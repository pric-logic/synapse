"""
MultimodalProcessor: Processes image, audio, and environmental data
Provides superior context understanding for delivery optimization
"""

import time
from typing import Dict, List, Any, Optional
from datetime import datetime
import random
import numpy as np
import cv2
from PIL import Image, ImageDraw, ImageFont
import io
import base64

from tools.api_simulator import APISimulator
from utils.gemini_client import get_gemini_client


class MultimodalProcessor:
    """
    Processes multimodal data inputs for comprehensive scenario analysis
    Combines visual, audio, and environmental intelligence
    """
    
    def __init__(self):
        self.api_simulator = APISimulator()
        self.processing_history = []
        self.accuracy_metrics = {
            'visual_accuracy': 0.94,
            'audio_accuracy': 0.89,
            'environmental_accuracy': 0.91,
            'fusion_accuracy': 0.94
        }
        
        # Initialize Gemini client for multimodal analysis
        try:
            self.gemini_client = get_gemini_client()
        except Exception as e:
            print(f"⚠️  Warning: Could not initialize Gemini client for multimodal processing: {e}")
            self.gemini_client = None
    
    def analyze_scenario(self, scenario: Any) -> Dict[str, Any]:
        """
        Analyze scenario using multimodal data fusion
        Returns comprehensive context for decision making
        """
        start_time = time.time()
        
        # Process different data types
        visual_data = self._process_visual_data(scenario)
        audio_data = self._process_audio_data(scenario)
        environmental_data = self._process_environmental_data(scenario)
        
        # Fuse all data sources
        fused_intelligence = self._fuse_multimodal_data(
            visual_data, audio_data, environmental_data, scenario
        )
        
        # Generate comprehensive context
        context = self._generate_context(fused_intelligence, scenario)
        
        # Record processing
        self._record_processing(scenario, context, time.time() - start_time)
        
        return context
    
    def _process_visual_data(self, scenario: Any) -> Dict[str, Any]:
        """Process visual data (traffic cameras, images)"""
        try:
            # Simulate traffic camera analysis
            if 'traffic' in scenario.description.lower() or 'accident' in scenario.description.lower():
                camera_data = self._analyze_traffic_camera(scenario)
            else:
                camera_data = self._generate_default_visual_data(scenario)
            
            # Process any additional images
            image_analysis = self._analyze_images(scenario)
            
            return {
                'traffic_camera': camera_data,
                'image_analysis': image_analysis,
                'visual_confidence': self.accuracy_metrics['visual_accuracy'],
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            print(f"Visual processing error: {e}")
            return self._generate_fallback_visual_data()
    
    def _analyze_traffic_camera(self, scenario: Any) -> Dict[str, Any]:
        """Analyze traffic camera feed"""
        # Simulate traffic camera data
        traffic_conditions = {
            'congestion_level': random.uniform(0.3, 0.9),
            'vehicle_count': random.randint(50, 200),
            'average_speed': random.uniform(5, 25),  # km/h
            'incident_detected': random.choice([True, False]),
            'road_conditions': random.choice(['clear', 'wet', 'congested', 'accident']),
            'visibility': random.uniform(0.6, 1.0)
        }
        
        # Generate synthetic traffic camera image
        image_data = self._generate_traffic_camera_image(traffic_conditions)
        
        return {
            'conditions': traffic_conditions,
            'image_data': image_data,
            'analysis_timestamp': datetime.now(),
            'camera_location': self._get_camera_location(scenario)
        }
    
    def _generate_traffic_camera_image(self, conditions: Dict[str, Any]) -> str:
        """Generate synthetic traffic camera image"""
        # Create a simple traffic visualization
        width, height = 640, 480
        image = Image.new('RGB', (width, height), color='lightblue')
        draw = ImageDraw.Draw(image)
        
        # Draw road
        road_color = 'darkgray' if conditions['road_conditions'] == 'wet' else 'gray'
        draw.rectangle([0, height//2-50, width, height//2+50], fill=road_color)
        
        # Draw traffic based on congestion
        vehicle_count = int(conditions['vehicle_count'] / 10)  # Scale down for visualization
        for i in range(vehicle_count):
            x = random.randint(0, width)
            y = random.randint(height//2-40, height//2+40)
            color = random.choice(['red', 'blue', 'green', 'yellow'])
            draw.ellipse([x, y, x+20, y+15], fill=color)
        
        # Add text overlay
        try:
            font = ImageFont.load_default()
            draw.text((10, 10), f"Congestion: {conditions['congestion_level']:.1%}", fill='black', font=font)
            draw.text((10, 30), f"Speed: {conditions['average_speed']:.1f} km/h", fill='black', font=font)
            draw.text((10, 50), f"Road: {conditions['road_conditions']}", fill='black', font=font)
        except:
            # Fallback if font loading fails
            draw.text((10, 10), f"Congestion: {conditions['congestion_level']:.1%}", fill='black')
            draw.text((10, 30), f"Speed: {conditions['average_speed']:.1f} km/h", fill='black')
            draw.text((10, 50), f"Road: {conditions['road_conditions']}", fill='black')
        
        # Convert to base64 for storage
        buffer = io.BytesIO()
        image.save(buffer, format='PNG')
        image_data = base64.b64encode(buffer.getvalue()).decode()
        
        return image_data
    
    def _get_camera_location(self, scenario: Any) -> Dict[str, float]:
        """Get camera location for the scenario"""
        # Simulate camera locations around the city
        base_lat, base_lng = 1.3521, 103.8198  # Singapore center
        
        # Add some variation based on scenario
        if 'highway' in scenario.description.lower():
            lat = base_lat + random.uniform(-0.02, 0.02)
            lng = base_lng + random.uniform(-0.02, 0.02)
        else:
            lat = base_lat + random.uniform(-0.01, 0.01)
            lng = base_lng + random.uniform(-0.01, 0.01)
        
        return {'lat': lat, 'lng': lng}
    
    def _generate_default_visual_data(self, scenario: Any) -> Dict[str, Any]:
        """Generate default visual data for non-traffic scenarios"""
        return {
            'conditions': {
                'congestion_level': random.uniform(0.1, 0.4),
                'vehicle_count': random.randint(10, 50),
                'average_speed': random.uniform(25, 40),
                'incident_detected': False,
                'road_conditions': 'clear',
                'visibility': random.uniform(0.8, 1.0)
            },
            'image_data': None,
            'analysis_timestamp': datetime.now(),
            'camera_location': self._get_camera_location(scenario)
        }
    
    def _analyze_images(self, scenario: Any) -> Dict[str, Any]:
        """Analyze any additional images in the scenario"""
        # Simulate image analysis
        base_analysis = {
            'objects_detected': [],
            'text_extracted': '',
            'image_quality': 'high',
            'analysis_confidence': 0.9
        }
        
        # If Gemini is available, enhance analysis
        if self.gemini_client and hasattr(scenario, 'raw_input') and scenario.raw_input:
            try:
                # Check if there are any image references in the scenario
                if any(img_ext in scenario.raw_input.lower() for img_ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    # Enhanced analysis with Gemini
                    enhanced_analysis = self._enhance_image_analysis_with_gemini(scenario)
                    base_analysis.update(enhanced_analysis)
            except Exception as e:
                print(f"Error in enhanced image analysis: {e}")
        
        return base_analysis
    
    def _enhance_image_analysis_with_gemini(self, scenario: Any) -> Dict[str, Any]:
        """Enhance image analysis using Gemini's multimodal capabilities"""
        try:
            # This would be used when actual images are provided
            # For now, return enhanced base analysis
            return {
                'gemini_enhanced': True,
                'analysis_model': 'gemini-1.5-pro',
                'enhanced_confidence': 0.95
            }
        except Exception as e:
            print(f"Error in Gemini-enhanced analysis: {e}")
            return {}
    
    def _generate_fallback_visual_data(self) -> Dict[str, Any]:
        """Generate fallback visual data if processing fails"""
        return {
            'traffic_camera': {
                'conditions': {
                    'congestion_level': 0.5,
                    'vehicle_count': 100,
                    'average_speed': 20,
                    'incident_detected': False,
                    'road_conditions': 'unknown',
                    'visibility': 0.8
                },
                'image_data': None,
                'analysis_timestamp': datetime.now(),
                'camera_location': {'lat': 1.3521, 'lng': 103.8198}
            },
            'image_analysis': {
                'objects_detected': [],
                'text_extracted': '',
                'image_quality': 'unknown',
                'analysis_confidence': 0.5
            },
            'visual_confidence': 0.5,
            'timestamp': datetime.now()
        }
    
    def _process_audio_data(self, scenario: Any) -> Dict[str, Any]:
        """Process audio data (driver voice, environmental sounds)"""
        try:
            # Analyze driver voice if available
            if 'driver' in scenario.description.lower() or 'voice' in scenario.description.lower():
                voice_analysis = self._analyze_driver_voice(scenario)
            else:
                voice_analysis = self._generate_default_voice_data()
            
            # Process environmental audio
            environmental_audio = self._analyze_environmental_audio(scenario)
            
            return {
                'voice_analysis': voice_analysis,
                'environmental_audio': environmental_audio,
                'audio_confidence': self.accuracy_metrics['audio_accuracy'],
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            print(f"Audio processing error: {e}")
            return self._generate_fallback_audio_data()
    
    def _analyze_driver_voice(self, scenario: Any) -> Dict[str, Any]:
        """Analyze driver voice for stress and emotion"""
        # Simulate voice analysis
        stress_level = random.uniform(0.3, 0.9)
        emotion = self._classify_emotion(stress_level)
        voice_quality = self._assess_voice_quality()
        
        return {
            'stress_level': stress_level,
            'emotion': emotion,
            'voice_quality': voice_quality,
            'speech_clarity': random.uniform(0.7, 1.0),
            'background_noise': random.uniform(0.1, 0.6),
            'analysis_confidence': 0.89
        }
    
    def _classify_emotion(self, stress_level: float) -> str:
        """Classify emotion based on stress level"""
        if stress_level > 0.8:
            return 'frustrated'
        elif stress_level > 0.6:
            return 'concerned'
        elif stress_level > 0.4:
            return 'neutral'
        else:
            return 'calm'
    
    def _assess_voice_quality(self) -> str:
        """Assess the quality of voice input"""
        quality_scores = ['poor', 'fair', 'good', 'excellent']
        weights = [0.1, 0.2, 0.4, 0.3]  # Bias towards good quality
        return random.choices(quality_scores, weights=weights)[0]
    
    def _generate_default_voice_data(self) -> Dict[str, Any]:
        """Generate default voice data when no driver voice is available"""
        return {
            'stress_level': random.uniform(0.2, 0.5),
            'emotion': 'neutral',
            'voice_quality': 'good',
            'speech_clarity': random.uniform(0.8, 1.0),
            'background_noise': random.uniform(0.1, 0.3),
            'analysis_confidence': 0.7
        }
    
    def _analyze_environmental_audio(self, scenario: Any) -> Dict[str, Any]:
        """Analyze environmental audio for context"""
        # Simulate environmental audio analysis
        return {
            'traffic_noise': random.uniform(0.3, 0.8),
            'construction_noise': random.uniform(0.0, 0.4),
            'weather_sounds': random.uniform(0.0, 0.6),
            'ambient_noise_level': random.uniform(0.2, 0.7),
            'audio_quality': 'good'
        }
    
    def _generate_fallback_audio_data(self) -> Dict[str, Any]:
        """Generate fallback audio data if processing fails"""
        return {
            'voice_analysis': {
                'stress_level': 0.5,
                'emotion': 'unknown',
                'voice_quality': 'unknown',
                'speech_clarity': 0.5,
                'background_noise': 0.5,
                'analysis_confidence': 0.5
            },
            'environmental_audio': {
                'traffic_noise': 0.5,
                'construction_noise': 0.0,
                'weather_sounds': 0.0,
                'ambient_noise_level': 0.5,
                'audio_quality': 'unknown'
            },
            'audio_confidence': 0.5,
            'timestamp': datetime.now()
        }
    
    def _process_environmental_data(self, scenario: Any) -> Dict[str, Any]:
        """Process environmental sensor data"""
        try:
            # Get weather data
            weather_data = self._get_weather_data(scenario)
            
            # Get traffic sensor data
            traffic_sensors = self._get_traffic_sensor_data(scenario)
            
            # Get air quality and other environmental factors
            environmental_factors = self._get_environmental_factors(scenario)
            
            return {
                'weather': weather_data,
                'traffic_sensors': traffic_sensors,
                'environmental_factors': environmental_factors,
                'environmental_confidence': self.accuracy_metrics['environmental_accuracy'],
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            print(f"Environmental processing error: {e}")
            return self._generate_fallback_environmental_data()
    
    def _get_weather_data(self, scenario: Any) -> Dict[str, Any]:
        """Get weather data for the scenario"""
        # Simulate weather API call
        weather_conditions = ['clear', 'cloudy', 'rain', 'storm', 'windy']
        current_weather = random.choice(weather_conditions)
        
        return {
            'condition': current_weather,
            'temperature': random.uniform(20, 35),
            'humidity': random.uniform(0.4, 0.9),
            'wind_speed': random.uniform(0, 25),
            'visibility': random.uniform(5, 20),
            'precipitation_probability': random.uniform(0, 1.0)
        }
    
    def _get_traffic_sensor_data(self, scenario: Any) -> Dict[str, Any]:
        """Get traffic sensor data"""
        return {
            'flow_rate': random.uniform(0.3, 1.0),
            'occupancy': random.uniform(0.2, 0.8),
            'speed_variance': random.uniform(5, 20),
            'incident_probability': random.uniform(0.1, 0.4)
        }
    
    def _get_environmental_factors(self, scenario: Any) -> Dict[str, Any]:
        """Get other environmental factors"""
        return {
            'air_quality_index': random.randint(20, 80),
            'noise_level': random.uniform(40, 80),
            'lighting_conditions': random.choice(['day', 'night', 'dusk', 'dawn']),
            'road_surface_condition': random.choice(['dry', 'wet', 'icy', 'rough'])
        }
    
    def _generate_fallback_environmental_data(self) -> Dict[str, Any]:
        """Generate fallback environmental data if processing fails"""
        return {
            'weather': {
                'condition': 'unknown',
                'temperature': 25,
                'humidity': 0.6,
                'wind_speed': 10,
                'visibility': 10,
                'precipitation_probability': 0.5
            },
            'traffic_sensors': {
                'flow_rate': 0.5,
                'occupancy': 0.5,
                'speed_variance': 10,
                'incident_probability': 0.2
            },
            'environmental_factors': {
                'air_quality_index': 50,
                'noise_level': 60,
                'lighting_conditions': 'day',
                'road_surface_condition': 'dry'
            },
            'environmental_confidence': 0.5,
            'timestamp': datetime.now()
        }
    
    def _fuse_multimodal_data(self, visual: Dict[str, Any], audio: Dict[str, Any], 
                              environmental: Dict[str, Any], scenario: Any) -> Dict[str, Any]:
        """Fuse all multimodal data sources into unified intelligence"""
        # Calculate fusion confidence
        fusion_confidence = (
            visual.get('visual_confidence', 0.5) * 0.4 +
            audio.get('audio_confidence', 0.5) * 0.3 +
            environmental.get('environmental_confidence', 0.5) * 0.3
        )
        
        # Identify conflicts and resolve them
        conflicts = self._identify_data_conflicts(visual, audio, environmental)
        resolved_data = self._resolve_conflicts(conflicts, visual, audio, environmental)
        
        # Generate unified intelligence
        unified_intelligence = {
            'traffic_analysis': self._analyze_traffic_intelligence(visual, environmental),
            'driver_wellbeing': self._analyze_driver_intelligence(audio, environmental),
            'environmental_impact': self._analyze_environmental_intelligence(visual, environmental),
            'risk_assessment': self._assess_overall_risk(visual, audio, environmental),
            'fusion_confidence': fusion_confidence,
            'data_quality': self._assess_data_quality(visual, audio, environmental),
            'timestamp': datetime.now()
        }
        
        return unified_intelligence
    
    def _identify_data_conflicts(self, visual: Dict[str, Any], audio: Dict[str, Any], 
                                environmental: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Identify conflicts between different data sources"""
        conflicts = []
        
        # Check for traffic conflicts
        visual_congestion = visual.get('traffic_camera', {}).get('conditions', {}).get('congestion_level', 0.5)
        env_traffic = environmental.get('traffic_sensors', {}).get('flow_rate', 0.5)
        
        if abs(visual_congestion - (1 - env_traffic)) > 0.3:
            conflicts.append({
                'type': 'traffic_conflict',
                'visual_value': visual_congestion,
                'environmental_value': env_traffic,
                'severity': 'medium'
            })
        
        return conflicts
    
    def _resolve_conflicts(self, conflicts: List[Dict[str, Any]], visual: Dict[str, Any], 
                          audio: Dict[str, Any], environmental: Dict[str, Any]) -> Dict[str, Any]:
        """Resolve data conflicts using confidence-weighted approach"""
        resolved = {}
        
        for conflict in conflicts:
            if conflict['type'] == 'traffic_conflict':
                # Use higher confidence source
                visual_conf = visual.get('visual_confidence', 0.5)
                env_conf = environmental.get('environmental_confidence', 0.5)
                
                if visual_conf > env_conf:
                    resolved['traffic_flow'] = visual.get('traffic_camera', {}).get('conditions', {}).get('congestion_level', 0.5)
                else:
                    resolved['traffic_flow'] = 1 - environmental.get('traffic_sensors', {}).get('flow_rate', 0.5)
        
        return resolved
    
    def _analyze_traffic_intelligence(self, visual: Dict[str, Any], environmental: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze traffic intelligence from visual and environmental data"""
        visual_traffic = visual.get('traffic_camera', {}).get('conditions', {})
        env_traffic = environmental.get('traffic_sensors', {})
        
        return {
            'congestion_level': visual_traffic.get('congestion_level', 0.5),
            'flow_efficiency': env_traffic.get('flow_rate', 0.5),
            'incident_probability': env_traffic.get('incident_probability', 0.2),
            'route_recommendation': self._generate_route_recommendation(visual_traffic, env_traffic)
        }
    
    def _analyze_driver_intelligence(self, audio: Dict[str, Any], environmental: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze driver wellbeing from audio and environmental data"""
        voice_analysis = audio.get('voice_analysis', {})
        env_factors = environmental.get('environmental_factors', {})
        
        return {
            'stress_level': voice_analysis.get('stress_level', 0.5),
            'emotional_state': voice_analysis.get('emotion', 'neutral'),
            'environmental_stressors': self._identify_stressors(env_factors),
            'support_recommendations': self._generate_support_recommendations(voice_analysis, env_factors)
        }
    
    def _analyze_environmental_intelligence(self, visual: Dict[str, Any], environmental: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze environmental impact on operations"""
        weather = environmental.get('weather', {})
        road_conditions = visual.get('traffic_camera', {}).get('conditions', {})
        
        return {
            'weather_impact': self._assess_weather_impact(weather),
            'road_conditions': road_conditions.get('road_conditions', 'unknown'),
            'operational_risks': self._identify_operational_risks(weather, road_conditions),
            'mitigation_strategies': self._generate_mitigation_strategies(weather, road_conditions)
        }
    
    def _assess_overall_risk(self, visual: Dict[str, Any], audio: Dict[str, Any], 
                             environmental: Dict[str, Any]) -> Dict[str, Any]:
        """Assess overall risk level from all data sources"""
        risk_factors = []
        
        # Traffic risk
        traffic_congestion = visual.get('traffic_camera', {}).get('conditions', {}).get('congestion_level', 0.5)
        if traffic_congestion > 0.7:
            risk_factors.append(('traffic_congestion', 'high'))
        elif traffic_congestion > 0.5:
            risk_factors.append(('traffic_congestion', 'medium'))
        
        # Driver risk
        driver_stress = audio.get('voice_analysis', {}).get('stress_level', 0.5)
        if driver_stress > 0.8:
            risk_factors.append(('driver_stress', 'high'))
        elif driver_stress > 0.6:
            risk_factors.append(('driver_stress', 'medium'))
        
        # Environmental risk
        weather = environmental.get('weather', {})
        if weather.get('condition') in ['storm', 'rain']:
            risk_factors.append(('weather', 'high'))
        
        # Calculate overall risk score
        risk_score = min(1.0, len([f for f in risk_factors if f[1] == 'high']) * 0.3 + 
                        len([f for f in risk_factors if f[1] == 'medium']) * 0.15)
        
        return {
            'risk_score': risk_score,
            'risk_level': 'high' if risk_score > 0.6 else 'medium' if risk_score > 0.3 else 'low',
            'risk_factors': risk_factors,
            'recommendations': self._generate_risk_recommendations(risk_factors)
        }
    
    def _assess_data_quality(self, visual: Dict[str, Any], audio: Dict[str, Any], 
                             environmental: Dict[str, Any]) -> Dict[str, Any]:
        """Assess the quality of all data sources"""
        return {
            'visual_quality': visual.get('visual_confidence', 0.5),
            'audio_quality': audio.get('audio_confidence', 0.5),
            'environmental_quality': environmental.get('environmental_confidence', 0.5),
            'overall_quality': (
                visual.get('visual_confidence', 0.5) * 0.4 +
                audio.get('audio_confidence', 0.5) * 0.3 +
                environmental.get('environmental_confidence', 0.5) * 0.3
            )
        }
    
    def _generate_context(self, fused_intelligence: Dict[str, Any], scenario: Any) -> Dict[str, Any]:
        """Generate comprehensive context for decision making"""
        return {
            'scenario_analysis': {
                'type': scenario.scenario_type,
                'severity': scenario.severity,
                'affected_deliveries': scenario.affected_deliveries,
                'description': scenario.description
            },
            'multimodal_intelligence': fused_intelligence,
            'recommendations': self._generate_recommendations(fused_intelligence, scenario),
            'confidence_metrics': {
                'overall_confidence': fused_intelligence.get('fusion_confidence', 0.5),
                'data_quality': fused_intelligence.get('data_quality', {}).get('overall_quality', 0.5),
                'risk_assessment': fused_intelligence.get('risk_assessment', {}).get('risk_score', 0.5)
            },
            'timestamp': datetime.now()
        }
    
    def _generate_recommendations(self, fused_intelligence: Dict[str, Any], scenario: Any) -> List[Dict[str, Any]]:
        """Generate actionable recommendations based on multimodal analysis"""
        recommendations = []
        
        # Traffic recommendations
        traffic_analysis = fused_intelligence.get('traffic_analysis', {})
        if traffic_analysis.get('congestion_level', 0.5) > 0.6:
            recommendations.append({
                'type': 'traffic_optimization',
                'priority': 'high',
                'description': 'Implement proactive rerouting for affected deliveries',
                'expected_impact': 'Reduce delivery delays by 15-25 minutes'
            })
        
        # Driver support recommendations
        driver_intelligence = fused_intelligence.get('driver_wellbeing', {})
        if driver_intelligence.get('stress_level', 0.5) > 0.7:
            recommendations.append({
                'type': 'driver_support',
                'priority': 'high',
                'description': 'Provide immediate driver assistance and support',
                'expected_impact': 'Improve driver performance and reduce risk'
            })
        
        # Environmental recommendations
        environmental_intelligence = fused_intelligence.get('environmental_impact', {})
        if environmental_intelligence.get('weather_impact', 'low') == 'high':
            recommendations.append({
                'type': 'weather_mitigation',
                'priority': 'medium',
                'description': 'Adjust delivery schedules and routes for weather conditions',
                'expected_impact': 'Maintain delivery reliability during adverse weather'
            })
        
        return recommendations
    
    def _generate_route_recommendation(self, visual_traffic: Dict[str, Any], env_traffic: Dict[str, Any]) -> str:
        """Generate route recommendation based on traffic data"""
        congestion = visual_traffic.get('congestion_level', 0.5)
        flow_rate = env_traffic.get('flow_rate', 0.5)
        
        if congestion > 0.7 or flow_rate < 0.3:
            return 'Use alternative routes to avoid congestion'
        elif congestion > 0.5 or flow_rate < 0.5:
            return 'Consider route optimization for better flow'
        else:
            return 'Current routes are optimal'
    
    def _identify_stressors(self, env_factors: Dict[str, Any]) -> List[str]:
        """Identify environmental stressors affecting drivers"""
        stressors = []
        
        if env_factors.get('noise_level', 50) > 70:
            stressors.append('High noise levels')
        if env_factors.get('air_quality_index', 50) > 60:
            stressors.append('Poor air quality')
        if env_factors.get('road_surface_condition') == 'rough':
            stressors.append('Poor road conditions')
        
        return stressors
    
    def _generate_support_recommendations(self, voice_analysis: Dict[str, Any], 
                                        env_factors: Dict[str, Any]) -> List[str]:
        """Generate support recommendations for drivers"""
        recommendations = []
        
        stress_level = voice_analysis.get('stress_level', 0.5)
        if stress_level > 0.8:
            recommendations.append('Immediate emotional support and counseling')
        elif stress_level > 0.6:
            recommendations.append('Regular check-ins and stress management resources')
        
        if env_factors.get('noise_level', 50) > 70:
            recommendations.append('Provide noise-canceling equipment')
        
        return recommendations
    
    def _assess_weather_impact(self, weather: Dict[str, Any]) -> str:
        """Assess weather impact on operations"""
        condition = weather.get('condition', 'clear')
        visibility = weather.get('visibility', 10)
        wind_speed = weather.get('wind_speed', 10)
        
        if condition in ['storm', 'rain'] and visibility < 8:
            return 'high'
        elif condition in ['rain', 'windy'] or wind_speed > 20:
            return 'medium'
        else:
            return 'low'
    
    def _identify_operational_risks(self, weather: Dict[str, Any], road_conditions: Dict[str, Any]) -> List[str]:
        """Identify operational risks from environmental factors"""
        risks = []
        
        if weather.get('condition') == 'storm':
            risks.append('Delivery delays due to severe weather')
        if road_conditions.get('road_conditions') == 'wet':
            risks.append('Increased accident risk on wet roads')
        if weather.get('visibility', 10) < 8:
            risks.append('Reduced visibility affecting driver safety')
        
        return risks
    
    def _generate_mitigation_strategies(self, weather: Dict[str, Any], road_conditions: Dict[str, Any]) -> List[str]:
        """Generate mitigation strategies for environmental risks"""
        strategies = []
        
        if weather.get('condition') == 'storm':
            strategies.append('Delay non-urgent deliveries')
            strategies.append('Provide weather updates to drivers')
        if road_conditions.get('road_conditions') == 'wet':
            strategies.append('Increase following distance requirements')
            strategies.append('Provide additional driver training')
        
        return strategies
    
    def _generate_risk_recommendations(self, risk_factors: List[tuple]) -> List[str]:
        """Generate recommendations based on identified risks"""
        recommendations = []
        
        for factor, level in risk_factors:
            if factor == 'traffic_congestion' and level == 'high':
                recommendations.append('Implement emergency rerouting protocols')
            elif factor == 'driver_stress' and level == 'high':
                recommendations.append('Provide immediate driver support and relief')
            elif factor == 'weather' and level == 'high':
                recommendations.append('Activate weather emergency protocols')
        
        return recommendations
    
    def _record_processing(self, scenario: Any, context: Dict[str, Any], processing_time: float):
        """Record multimodal processing for analysis"""
        record = {
            'timestamp': datetime.now(),
            'scenario_id': scenario.id,
            'scenario_type': scenario.scenario_type,
            'processing_time': processing_time,
            'fusion_confidence': context.get('multimodal_intelligence', {}).get('fusion_confidence', 0.5),
            'data_quality': context.get('confidence_metrics', {}).get('data_quality', 0.5)
        }
        
        self.processing_history.append(record)
        
        # Keep only last 1000 records
        if len(self.processing_history) > 1000:
            self.processing_history = self.processing_history[-1000:]
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """Get multimodal processing statistics"""
        if not self.processing_history:
            return self.accuracy_metrics
        
        recent_records = self.processing_history[-100:] if len(self.processing_history) > 100 else self.processing_history
        
        avg_confidence = sum(r['fusion_confidence'] for r in recent_records) / len(recent_records)
        avg_quality = sum(r['data_quality'] for r in recent_records) / len(recent_records)
        avg_processing_time = sum(r['processing_time'] for r in recent_records) / len(recent_records)
        
        return {
            **self.accuracy_metrics,
            'recent_fusion_confidence': avg_confidence,
            'recent_data_quality': avg_quality,
            'average_processing_time': avg_processing_time,
            'total_processed': len(self.processing_history)
        } 